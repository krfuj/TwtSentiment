{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a47f1ef",
   "metadata": {
    "id": "4a47f1ef"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22b7cc4",
   "metadata": {
    "id": "c22b7cc4"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"twitter_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c4bcf9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "6c4bcf9c",
    "outputId": "1bb0865a-6758-430c-e919-c9c0761b1c31",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiments</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9200</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id sentiments                                             tweets\n",
       "0          2401   Positive  I am coming to the borders and I will kill you...\n",
       "1          2401   Positive  im getting on borderlands and i will kill you ...\n",
       "2          2401   Positive  im coming on borderlands and i will murder you...\n",
       "3          2401   Positive  im getting on borderlands 2 and i will murder ...\n",
       "4          2401   Positive  im getting into borderlands and i can murder y...\n",
       "...         ...        ...                                                ...\n",
       "74676      9200   Positive  Just realized that the Windows partition of my...\n",
       "74677      9200   Positive  Just realized that my Mac window partition is ...\n",
       "74678      9200   Positive  Just realized the windows partition of my Mac ...\n",
       "74679      9200   Positive  Just realized between the windows partition of...\n",
       "74680      9200   Positive  Just like the windows partition of my Mac is l...\n",
       "\n",
       "[74681 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label the columns\n",
    "df.columns = [\"tweet_id\",\"place\", \"sentiments\", \"tweets\"]\n",
    "df = df.drop(\"place\", axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f502908a",
   "metadata": {
    "id": "f502908a"
   },
   "outputs": [],
   "source": [
    "# df_twt = [df[\"tweets\"]]\n",
    "# df_twt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5568a",
   "metadata": {
    "id": "4fb5568a"
   },
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21afdf30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21afdf30",
    "outputId": "2e77c038-2576-4a0e-8332-673cbea53ec8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id    19210.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify outliers\n",
    "outliers = df.describe().loc[\"75%\"] + 1.5 * (df.describe().loc[\"75%\"] - df.describe().loc[\"25%\"])\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2228d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "3d2228d0",
    "outputId": "d9b70fa1-88db-4d4c-d4a7-d1fea774b12b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiments</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2404</td>\n",
       "      <td>Positive</td>\n",
       "      <td>that was the first borderlands session in a lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2405</td>\n",
       "      <td>Negative</td>\n",
       "      <td>The biggest disappointment of my life came a y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2409</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Blaming Sight for Tardiness! A little bit of b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2411</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>.. [</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2425</td>\n",
       "      <td>Negative</td>\n",
       "      <td>\"What a bitch!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74504</th>\n",
       "      <td>9171</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>This benchmarking comparison between Oculus Qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74509</th>\n",
       "      <td>9172</td>\n",
       "      <td>Positive</td>\n",
       "      <td>@ NVIDIAGeForce @ nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74588</th>\n",
       "      <td>9185</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Heard people are having issues with ordering t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74618</th>\n",
       "      <td>9190</td>\n",
       "      <td>Positive</td>\n",
       "      <td>This news about the Nvidia 3000 series is ligi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74635</th>\n",
       "      <td>9193</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Love EVERYTHING about it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id sentiments                                             tweets\n",
       "19         2404   Positive  that was the first borderlands session in a lo...\n",
       "25         2405   Negative  The biggest disappointment of my life came a y...\n",
       "50         2409    Neutral  Blaming Sight for Tardiness! A little bit of b...\n",
       "63         2411    Neutral                                               .. [\n",
       "145        2425   Negative                                    \"What a bitch!\"\n",
       "...         ...        ...                                                ...\n",
       "74504      9171    Neutral  This benchmarking comparison between Oculus Qu...\n",
       "74509      9172   Positive                           @ NVIDIAGeForce @ nvidia\n",
       "74588      9185    Neutral  Heard people are having issues with ordering t...\n",
       "74618      9190   Positive  This news about the Nvidia 3000 series is ligi...\n",
       "74635      9193   Positive                          Love EVERYTHING about it.\n",
       "\n",
       "[2700 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify inconsistencies\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f9f4465",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f9f4465",
    "outputId": "0f3c8ec4-b812-4219-94d3-fe7558cad6d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id        0\n",
       "sentiments      0\n",
       "tweets        686\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify missing values\n",
    "missing_values = df.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91af7f4",
   "metadata": {
    "id": "c91af7f4"
   },
   "outputs": [],
   "source": [
    "# Remove the outlier value\n",
    "df = df[df[\"tweet_id\"] != 19210.0]\n",
    "\n",
    "# removing duplicates\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "# Remove the duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#This will remove any character in the ‘tweets’ column that is not a letter (upper or lower case), a number, a space, or a punctuation character (.,;?!$%^&*()-_+=[]{}'|\"<>`~) with an empty string\n",
    "df['tweets'] = df['tweets'].astype(str)\n",
    "df['tweets'] = df['tweets'].str.replace('[^a-zA-Z0-9\\s.,;?!$%^&*()-_+=[]{}\\'\"|<>`~]', '', regex=True)\n",
    "\n",
    "\n",
    "#fill 0 in mum\n",
    "df['tweets'] = df['tweets'].fillna(0)\n",
    "\n",
    "# Save the dataframe\n",
    "df.to_csv(\"tweet_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4b79cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef4b79cc",
    "outputId": "89856a17-4526-4d78-e5b0-f89b8707d351"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a392a91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a392a91",
    "outputId": "da7f7227-3f96-403a-c143-c5ce627b7fb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check null\n",
    "df['tweets'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c26aabff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c26aabff",
    "outputId": "c65aded5-eda6-4ee6-9800-e5495368d418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "special_chars = df['tweets'].str.contains('[^a-zA-Z0-9\\s.,;?!$%^&*()-_+=[]{}\\'\"|<>`~]')\n",
    "print(any(special_chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6441384",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6441384",
    "outputId": "8d085db8-3338-48ff-c4e3-db7792edf4ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 71981 entries, 0 to 74680\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweet_id    71981 non-null  int64 \n",
      " 1   sentiments  71981 non-null  object\n",
      " 2   tweets      71981 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# the number of missing values and the data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4a543",
   "metadata": {
    "id": "00b4a543"
   },
   "outputs": [],
   "source": [
    "# df_train_POS = df[df[\"Positive\"] == \"Positive\"]\n",
    "# df_train_NEG = df_train[df_train[\"Positive\"] == \"Negative\"]\n",
    "# df_train_NEU = df_train[df_train[\"Positive\"] == \"Neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f6f57",
   "metadata": {
    "id": "265f6f57"
   },
   "outputs": [],
   "source": [
    "# df_train_POS = df_train_POS.sample(15000)\n",
    "# df_train_NEG = df_train_NEG.sample(15000)\n",
    "# df_train_NEU = df_train_NEU.sample(15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a30133",
   "metadata": {
    "id": "a8a30133"
   },
   "outputs": [],
   "source": [
    "# df_train_POS.shape, df_train_NEG.shape, df_train_NEU.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609e972",
   "metadata": {
    "id": "0609e972"
   },
   "source": [
    "# Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GIlYYflUrNde",
   "metadata": {
    "id": "GIlYYflUrNde"
   },
   "source": [
    "Analysing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "jTbXXNeEBUrc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTbXXNeEBUrc",
    "outputId": "c2f7ad02-8954-4c29-8a48-d81e35a7153a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8be3b3de",
   "metadata": {
    "id": "8be3b3de"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faWvKeiptLl1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "faWvKeiptLl1",
    "outputId": "61165177-c496-4156-c15c-8fb021fc43ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the number of unique classes in your labels\n",
    "# num_labels = len(np.unique(labels))\n",
    "\n",
    "# Instantiate the BERT model and tokenizer\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "QFEX2FKg_ZJl",
   "metadata": {
    "id": "QFEX2FKg_ZJl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the precision and recall metrics\n",
    "precision = Precision()\n",
    "recall = Recall()\n",
    "\n",
    "# Prepare the inputs for the model\n",
    "input_ids, attention_masks, encoded_labels = [], [], []  # Create a new list for your encoded labels\n",
    "\n",
    "# Define labels as a list of sentiment labels from your DataFrame\n",
    "labels = df['sentiments'].values.tolist()\n",
    "\n",
    "# Prepare the inputs for the model\n",
    "# input_ids, attention_masks, labels = [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "O7oldmaSYzLA",
   "metadata": {
    "id": "O7oldmaSYzLA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "t1W4_4zi_eQA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "t1W4_4zi_eQA",
    "outputId": "0fcd84a9-03b9-407c-be68-905b277c0beb"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7ae7af508543>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, pad_to_max_length=True, \n\u001b[1;32m      3\u001b[0m                                    return_attention_mask=True, return_token_type_ids=True)\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mattention_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mnp_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_numpy_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m       \"\"\")\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "for tweet, label in zip(df['tweets'], df['sentiments']):\n",
    "    inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, pad_to_max_length=True,\n",
    "                                   return_attention_mask=True, return_token_type_ids=True)\n",
    "    input_ids.append(inputs['input_ids'])\n",
    "    attention_masks.append(inputs['attention_mask'])\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "zf-XdazS_gyy",
   "metadata": {
    "id": "zf-XdazS_gyy"
   },
   "outputs": [],
   "source": [
    "# Convert lists to tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "\n",
    "# Convert string labels to integers\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "# Convert integer labels to one-hot encoded format\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zhN5bjea_kkT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhN5bjea_kkT",
    "outputId": "480d3fb7-f0d0-438e-8b32-21b2247bbd76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy('accuracy'), precision, recall])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([input_ids, attention_masks], labels, batch_size=100, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FpXhH08x59LI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpXhH08x59LI",
    "outputId": "19afb861-3269-4c0b-9358-267b5486ee90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "   3/2250 [..............................] - ETA: 31:45:33 - loss: 1.3801 - accuracy: 0.3333 - precision_5: 0.5625 - recall_5: 0.0938"
     ]
    }
   ],
   "source": [
    "# Print the history\n",
    "print(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lpspXUb5aQNf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpspXUb5aQNf",
    "outputId": "bbc5e75a-5551-416c-b6dd-f261ae848939"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define labels as a list of sentiment labels from your DataFrame\n",
    "labels = df['sentiments'].values.tolist()\n",
    "\n",
    "# Define the number of unique classes in your labels\n",
    "num_labels = len(np.unique(labels))\n",
    "\n",
    "# Instantiate the BERT model and tokenizer\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Instantiate the precision and recall metrics\n",
    "precision = Precision()\n",
    "recall = Recall()\n",
    "\n",
    "# Prepare the inputs for the model\n",
    "input_ids, attention_masks, encoded_labels = [], [], []  # Create a new list for your encoded labels\n",
    "\n",
    "for tweet, label in zip(df['tweets'], labels):\n",
    "    inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, pad_to_max_length=True,\n",
    "                                   return_attention_mask=True, return_token_type_ids=True)\n",
    "    input_ids.append(inputs['input_ids'])\n",
    "    attention_masks.append(inputs['attention_mask'])\n",
    "    encoded_labels.append(label)  # Append to encoded_labels instead of labels\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "\n",
    "# Convert string labels to integers\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(encoded_labels)  # Use encoded_labels here\n",
    "\n",
    "# Convert integer labels to one-hot encoded format\n",
    "encoded_labels = to_categorical(encoded_labels)  # And here\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy('accuracy'), precision, recall])\n",
    "\n",
    "\n",
    "#Calculate batch size\n",
    "batch_size = len(labels) // 25\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([input_ids, attention_masks], encoded_labels, batch_size=batch_size, epochs=2)\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit([input_ids, attention_masks], encoded_labels, batch_size=32, epochs=2)  # And here\n",
    "\n",
    "# Print the history\n",
    "print(history.history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MMo3bUOhbQ4t",
   "metadata": {
    "id": "MMo3bUOhbQ4t"
   },
   "source": [
    "##BARD2\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall, SparseCategoricalAccuracy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the number of unique classes in your labels\n",
    "num_labels = len(np.unique(labels))\n",
    "\n",
    "# Instantiate the BERT model and tokenizer\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Instantiate the precision and recall metrics\n",
    "precision = Precision()\n",
    "recall = Recall()\n",
    "\n",
    "# Prepare the inputs for the model\n",
    "input_ids, attention_masks, labels = [], [], []\n",
    "\n",
    "for tweet, label in zip(df['tweets'], df['sentiments']):\n",
    "    inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, pad_to_max_length=True,\n",
    "                                   return_attention_mask=True, return_token_type_ids=True)\n",
    "    input_ids.append(inputs['input_ids'])\n",
    "    attention_masks.append(inputs['attention_mask'])\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "\n",
    "# Convert string labels to integers\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "# Convert integer labels to one-hot encoded format\n",
    "# (not needed with sparse_categorical_accuracy)\n",
    "# labels = to_categorical(labels)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[SparseCategoricalAccuracy('accuracy'), precision, recall])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([input_ids, attention_masks], labels, batch_size=32, epochs=2)\n",
    "\n",
    "# Print the history\n",
    "print(history.history)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
